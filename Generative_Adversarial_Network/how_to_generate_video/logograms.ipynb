{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th') # ensure our dimension notation matches\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, AveragePooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import mnist\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*8*8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((128, 8, 8), input_shape=(128*8*8,)))\n",
    "    model.add(UpSampling2D(size=(4, 4)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(4, 4)))\n",
    "    model.add(Convolution2D(1, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(\n",
    "                        64, 5, 5,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(1, 128, 128)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 4)))\n",
    "    model.add(Convolution2D(128, 5, 5))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[0, :, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  app.launch_new_instance()\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8192)              8396800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 32, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 32, 32)        204864    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 64, 128, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 128, 128)       1601      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 128, 128)       0         \n",
      "=================================================================\n",
      "Total params: 8,739,457.0\n",
      "Trainable params: 8,723,073.0\n",
      "Non-trainable params: 16,384.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = generator_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data(pixels=128, verbose=False):\n",
    "    print(\"Loading data\")\n",
    "    X_train = []\n",
    "    paths = glob.glob(os.path.normpath(os.getcwd() + '/logos/*.jpg'))\n",
    "    for path in paths:\n",
    "        if verbose: print(path)\n",
    "        im = Image.open(path)\n",
    "        im = ImageOps.fit(im, (pixels, pixels), Image.ANTIALIAS)\n",
    "        im = ImageOps.grayscale(im)\n",
    "        #im.show()\n",
    "        im = np.asarray(im)\n",
    "        X_train.append(im)\n",
    "    print(\"Finished loading data\")\n",
    "    return np.array(X_train)\n",
    "\n",
    "def train(epochs, BATCH_SIZE, weights=False):\n",
    "    \"\"\"\n",
    "    :param epochs: Train for this many epochs\n",
    "    :param BATCH_SIZE: Size of minibatch\n",
    "    :param weights: If True, load weights from file, otherwise train the model from scratch. \n",
    "    Use this if you have already saved state of the network and want to train it further.\n",
    "    \"\"\"\n",
    "    X_train = load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:])\n",
    "    discriminator = discriminator_model()\n",
    "    generator = generator_model()\n",
    "    if weights:\n",
    "        generator.load_weights('goodgenerator.h5')\n",
    "        discriminator.load_weights('gooddiscriminator.h5')\n",
    "    discriminator_on_generator = \\\n",
    "        generator_containing_discriminator(generator, discriminator)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    discriminator_on_generator.compile(\n",
    "        loss='binary_crossentropy', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            #print(generated_images.shape)\n",
    "            if index % 20 == 0 and epoch % 10 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                destpath = os.path.normpath(os.getcwd()+ \"/logo-generated-images/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "                Image.fromarray(image.astype(np.uint8)).save(destpath)\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, [1] * BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if epoch % 10 == 9:\n",
    "                generator.save_weights('goodgenerator.h5', True)\n",
    "                discriminator.save_weights('gooddiscriminator.h5', True)\n",
    "\n",
    "def clean(image):\n",
    "    for i in range(1, image.shape[0] - 1):\n",
    "        for j in range(1, image.shape[1] - 1):\n",
    "            if image[i][j] + image[i+1][j] + image[i][j+1] + image[i-1][j] + image[i][j-1] > 127 * 5:\n",
    "                image[i][j] = 255\n",
    "    return image\n",
    "def generate(BATCH_SIZE):\n",
    "    generator = generator_model()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    generator.load_weights('goodgenerator.h5')\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    a = np.random.uniform(-1, 1, 100)\n",
    "    b = np.random.uniform(-1, 1, 100)\n",
    "    grad = (b - a) / BATCH_SIZE\n",
    "    for i in range(BATCH_SIZE):\n",
    "        noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "    generated_images = generator.predict(noise, verbose=1)\n",
    "    #image = combine_images(generated_images)\n",
    "    print(generated_images.shape)\n",
    "    for image in generated_images:\n",
    "        image = image[0]\n",
    "        image = image*127.5+127.5\n",
    "        Image.fromarray(image.astype(np.uint8)).save(\"dirty.png\")\n",
    "        Image.fromarray(image.astype(np.uint8)).show()\n",
    "        clean(image)\n",
    "        image = Image.fromarray(image.astype(np.uint8))\n",
    "        image.show()        \n",
    "        image.save(\"clean.png\")\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Finished loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), input_shape=(1, 128, 1..., padding=\"same\")`\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5))`\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  app.launch_new_instance()\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.682582\n",
      "batch 0 g_loss : 0.684046\n",
      "batch 1 d_loss : 0.635724\n",
      "batch 1 g_loss : 0.686342\n",
      "batch 2 d_loss : 0.579866\n",
      "batch 2 g_loss : 0.707961\n",
      "Epoch is 1\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.547933\n",
      "batch 0 g_loss : 0.678845\n",
      "batch 1 d_loss : 0.487530\n",
      "batch 1 g_loss : 0.698422\n",
      "batch 2 d_loss : 0.451920\n",
      "batch 2 g_loss : 0.700883\n",
      "Epoch is 2\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.439300\n",
      "batch 0 g_loss : 0.692629\n",
      "batch 1 d_loss : 0.417594\n",
      "batch 1 g_loss : 0.729387\n",
      "batch 2 d_loss : 0.375918\n",
      "batch 2 g_loss : 0.676392\n",
      "Epoch is 3\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.388842\n",
      "batch 0 g_loss : 0.703631\n",
      "batch 1 d_loss : 0.400287\n",
      "batch 1 g_loss : 0.676074\n",
      "batch 2 d_loss : 0.389793\n",
      "batch 2 g_loss : 0.684802\n",
      "Epoch is 4\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.424666\n",
      "batch 0 g_loss : 0.685858\n",
      "batch 1 d_loss : 0.402327\n",
      "batch 1 g_loss : 0.675226\n",
      "batch 2 d_loss : 0.416446\n",
      "batch 2 g_loss : 0.662618\n",
      "Epoch is 5\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.446790\n",
      "batch 0 g_loss : 0.655154\n",
      "batch 1 d_loss : 0.491038\n",
      "batch 1 g_loss : 0.674441\n",
      "batch 2 d_loss : 0.488302\n",
      "batch 2 g_loss : 0.609840\n",
      "Epoch is 6\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.501363\n",
      "batch 0 g_loss : 0.624631\n",
      "batch 1 d_loss : 0.563810\n",
      "batch 1 g_loss : 0.590891\n",
      "batch 2 d_loss : 0.583022\n",
      "batch 2 g_loss : 0.612594\n",
      "Epoch is 7\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.571184\n",
      "batch 0 g_loss : 0.649757\n",
      "batch 1 d_loss : 0.561772\n",
      "batch 1 g_loss : 0.599660\n",
      "batch 2 d_loss : 0.585666\n",
      "batch 2 g_loss : 0.638970\n",
      "Epoch is 8\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.576700\n",
      "batch 0 g_loss : 0.578634\n",
      "batch 1 d_loss : 0.578750\n",
      "batch 1 g_loss : 0.615151\n",
      "batch 2 d_loss : 0.562166\n",
      "batch 2 g_loss : 0.635990\n",
      "Epoch is 9\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.545664\n",
      "batch 0 g_loss : 0.651173\n",
      "batch 1 d_loss : 0.529968\n",
      "batch 1 g_loss : 0.677018\n",
      "batch 2 d_loss : 0.530370\n",
      "batch 2 g_loss : 0.704810\n",
      "Epoch is 10\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.529955\n",
      "batch 0 g_loss : 0.700649\n",
      "batch 1 d_loss : 0.509591\n",
      "batch 1 g_loss : 0.726876\n",
      "batch 2 d_loss : 0.490721\n",
      "batch 2 g_loss : 0.747804\n",
      "Epoch is 11\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.501602\n",
      "batch 0 g_loss : 0.772579\n",
      "batch 1 d_loss : 0.487721\n",
      "batch 1 g_loss : 0.810255\n",
      "batch 2 d_loss : 0.481321\n",
      "batch 2 g_loss : 0.800086\n",
      "Epoch is 12\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.489211\n",
      "batch 0 g_loss : 0.819968\n",
      "batch 1 d_loss : 0.476855\n",
      "batch 1 g_loss : 0.863582\n",
      "batch 2 d_loss : 0.463409\n",
      "batch 2 g_loss : 0.852377\n",
      "Epoch is 13\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.477565\n",
      "batch 0 g_loss : 0.883745\n",
      "batch 1 d_loss : 0.475220\n",
      "batch 1 g_loss : 0.904500\n",
      "batch 2 d_loss : 0.459521\n",
      "batch 2 g_loss : 0.931747\n",
      "Epoch is 14\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.460523\n",
      "batch 0 g_loss : 0.924312\n",
      "batch 1 d_loss : 0.456788\n",
      "batch 1 g_loss : 0.923232\n",
      "batch 2 d_loss : 0.443945\n",
      "batch 2 g_loss : 0.931813\n",
      "Epoch is 15\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.438878\n",
      "batch 0 g_loss : 0.944804\n",
      "batch 1 d_loss : 0.441713\n",
      "batch 1 g_loss : 0.984927\n",
      "batch 2 d_loss : 0.418286\n",
      "batch 2 g_loss : 0.959833\n",
      "Epoch is 16\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.419588\n",
      "batch 0 g_loss : 0.989780\n",
      "batch 1 d_loss : 0.418666\n",
      "batch 1 g_loss : 0.970797\n",
      "batch 2 d_loss : 0.394893\n",
      "batch 2 g_loss : 0.970097\n",
      "Epoch is 17\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.397132\n",
      "batch 0 g_loss : 0.977365\n",
      "batch 1 d_loss : 0.391316\n",
      "batch 1 g_loss : 0.987738\n",
      "batch 2 d_loss : 0.363517\n",
      "batch 2 g_loss : 1.027245\n",
      "Epoch is 18\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.385033\n",
      "batch 0 g_loss : 1.008909\n",
      "batch 1 d_loss : 0.379295\n",
      "batch 1 g_loss : 1.056522\n",
      "batch 2 d_loss : 0.343974\n",
      "batch 2 g_loss : 1.063893\n",
      "Epoch is 19\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.352300\n",
      "batch 0 g_loss : 1.100404\n",
      "batch 1 d_loss : 0.352547\n",
      "batch 1 g_loss : 1.108470\n",
      "batch 2 d_loss : 0.323963\n",
      "batch 2 g_loss : 1.053704\n",
      "Epoch is 20\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.331865\n",
      "batch 0 g_loss : 1.072973\n",
      "batch 1 d_loss : 0.319603\n",
      "batch 1 g_loss : 1.161518\n",
      "batch 2 d_loss : 0.290798\n",
      "batch 2 g_loss : 1.222805\n",
      "Epoch is 21\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.310993\n",
      "batch 0 g_loss : 1.214848\n",
      "batch 1 d_loss : 0.303777\n",
      "batch 1 g_loss : 1.198540\n",
      "batch 2 d_loss : 0.265892\n",
      "batch 2 g_loss : 1.210400\n",
      "Epoch is 22\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.278920\n",
      "batch 0 g_loss : 1.231660\n",
      "batch 1 d_loss : 0.282313\n",
      "batch 1 g_loss : 1.309623\n",
      "batch 2 d_loss : 0.266941\n",
      "batch 2 g_loss : 1.318333\n",
      "Epoch is 23\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.269227\n",
      "batch 0 g_loss : 1.304428\n",
      "batch 1 d_loss : 0.259849\n",
      "batch 1 g_loss : 1.390499\n",
      "batch 2 d_loss : 0.233313\n",
      "batch 2 g_loss : 1.392875\n",
      "Epoch is 24\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.248368\n",
      "batch 0 g_loss : 1.364834\n",
      "batch 1 d_loss : 0.248799\n",
      "batch 1 g_loss : 1.387600\n",
      "batch 2 d_loss : 0.209110\n",
      "batch 2 g_loss : 1.432733\n",
      "Epoch is 25\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.234497\n",
      "batch 0 g_loss : 1.472230\n",
      "batch 1 d_loss : 0.235850\n",
      "batch 1 g_loss : 1.421494\n",
      "batch 2 d_loss : 0.204786\n",
      "batch 2 g_loss : 1.402320\n",
      "Epoch is 26\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.213270\n",
      "batch 0 g_loss : 1.426979\n",
      "batch 1 d_loss : 0.221791\n",
      "batch 1 g_loss : 1.448045\n",
      "batch 2 d_loss : 0.182766\n",
      "batch 2 g_loss : 1.422986\n",
      "Epoch is 27\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.208669\n",
      "batch 0 g_loss : 1.485339\n",
      "batch 1 d_loss : 0.213599\n",
      "batch 1 g_loss : 1.517864\n",
      "batch 2 d_loss : 0.162646\n",
      "batch 2 g_loss : 1.504294\n",
      "Epoch is 28\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.190775\n",
      "batch 0 g_loss : 1.580660\n",
      "batch 1 d_loss : 0.209666\n",
      "batch 1 g_loss : 1.583532\n",
      "batch 2 d_loss : 0.151689\n",
      "batch 2 g_loss : 1.694369\n",
      "Epoch is 29\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.187544\n",
      "batch 0 g_loss : 1.607290\n",
      "batch 1 d_loss : 0.171786\n",
      "batch 1 g_loss : 1.462800\n",
      "batch 2 d_loss : 0.142922\n",
      "batch 2 g_loss : 1.501798\n",
      "Epoch is 30\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.192817\n",
      "batch 0 g_loss : 1.564804\n",
      "batch 1 d_loss : 0.178372\n",
      "batch 1 g_loss : 1.737431\n",
      "batch 2 d_loss : 0.129131\n",
      "batch 2 g_loss : 1.659552\n",
      "Epoch is 31\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.154038\n",
      "batch 0 g_loss : 1.798527\n",
      "batch 1 d_loss : 0.169493\n",
      "batch 1 g_loss : 1.881689\n",
      "batch 2 d_loss : 0.119862\n",
      "batch 2 g_loss : 1.695823\n",
      "Epoch is 32\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.148709\n",
      "batch 0 g_loss : 1.790870\n",
      "batch 1 d_loss : 0.158079\n",
      "batch 1 g_loss : 1.557250\n",
      "batch 2 d_loss : 0.124628\n",
      "batch 2 g_loss : 1.684599\n",
      "Epoch is 33\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.158438\n",
      "batch 0 g_loss : 1.796047\n",
      "batch 1 d_loss : 0.158738\n",
      "batch 1 g_loss : 1.733000\n",
      "batch 2 d_loss : 0.139014\n",
      "batch 2 g_loss : 1.604253\n",
      "Epoch is 34\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.170832\n",
      "batch 0 g_loss : 1.729254\n",
      "batch 1 d_loss : 0.181704\n",
      "batch 1 g_loss : 1.792030\n",
      "batch 2 d_loss : 0.121909\n",
      "batch 2 g_loss : 1.765285\n",
      "Epoch is 35\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.176103\n",
      "batch 0 g_loss : 1.613263\n",
      "batch 1 d_loss : 0.174521\n",
      "batch 1 g_loss : 1.620365\n",
      "batch 2 d_loss : 0.126102\n",
      "batch 2 g_loss : 1.958748\n",
      "Epoch is 36\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.165145\n",
      "batch 0 g_loss : 1.931834\n",
      "batch 1 d_loss : 0.168224\n",
      "batch 1 g_loss : 1.749234\n",
      "batch 2 d_loss : 0.127835\n",
      "batch 2 g_loss : 1.842842\n",
      "Epoch is 37\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.147946\n",
      "batch 0 g_loss : 1.629184\n",
      "batch 1 d_loss : 0.158937\n",
      "batch 1 g_loss : 1.673973\n",
      "batch 2 d_loss : 0.142314\n",
      "batch 2 g_loss : 1.801677\n",
      "Epoch is 38\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.146678\n",
      "batch 0 g_loss : 1.714443\n",
      "batch 1 d_loss : 0.210545\n",
      "batch 1 g_loss : 1.552745\n",
      "batch 2 d_loss : 0.114746\n",
      "batch 2 g_loss : 1.722840\n",
      "Epoch is 39\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.174821\n",
      "batch 0 g_loss : 1.532784\n",
      "batch 1 d_loss : 0.181170\n",
      "batch 1 g_loss : 1.353094\n",
      "batch 2 d_loss : 0.130829\n",
      "batch 2 g_loss : 1.426790\n",
      "Epoch is 40\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.201734\n",
      "batch 0 g_loss : 2.096884\n",
      "batch 1 d_loss : 0.270668\n",
      "batch 1 g_loss : 1.351349\n",
      "batch 2 d_loss : 0.191284\n",
      "batch 2 g_loss : 1.540215\n",
      "Epoch is 41\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.219497\n",
      "batch 0 g_loss : 1.761892\n",
      "batch 1 d_loss : 0.271121\n",
      "batch 1 g_loss : 1.811729\n",
      "batch 2 d_loss : 0.164828\n",
      "batch 2 g_loss : 1.634221\n",
      "Epoch is 42\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.216357\n",
      "batch 0 g_loss : 1.827109\n",
      "batch 1 d_loss : 0.268926\n",
      "batch 1 g_loss : 1.582057\n",
      "batch 2 d_loss : 0.114064\n",
      "batch 2 g_loss : 1.527935\n",
      "Epoch is 43\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.304553\n",
      "batch 0 g_loss : 1.593555\n",
      "batch 1 d_loss : 0.331247\n",
      "batch 1 g_loss : 1.937349\n",
      "batch 2 d_loss : 0.206885\n",
      "batch 2 g_loss : 2.054007\n",
      "Epoch is 44\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.227795\n",
      "batch 0 g_loss : 1.975615\n",
      "batch 1 d_loss : 0.301898\n",
      "batch 1 g_loss : 1.797471\n",
      "batch 2 d_loss : 0.248182\n",
      "batch 2 g_loss : 1.841734\n",
      "Epoch is 45\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.231849\n",
      "batch 0 g_loss : 2.064841\n",
      "batch 1 d_loss : 0.377694\n",
      "batch 1 g_loss : 2.038723\n",
      "batch 2 d_loss : 0.253338\n",
      "batch 2 g_loss : 1.966534\n",
      "Epoch is 46\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.231585\n",
      "batch 0 g_loss : 1.785029\n",
      "batch 1 d_loss : 0.402042\n",
      "batch 1 g_loss : 1.899696\n",
      "batch 2 d_loss : 0.264607\n",
      "batch 2 g_loss : 1.755925\n",
      "Epoch is 47\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.292767\n",
      "batch 0 g_loss : 1.731082\n",
      "batch 1 d_loss : 0.464200\n",
      "batch 1 g_loss : 1.719857\n",
      "batch 2 d_loss : 0.364935\n",
      "batch 2 g_loss : 2.034502\n",
      "Epoch is 48\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.359826\n",
      "batch 0 g_loss : 1.478744\n",
      "batch 1 d_loss : 0.372380\n",
      "batch 1 g_loss : 1.897560\n",
      "batch 2 d_loss : 0.310633\n",
      "batch 2 g_loss : 2.062506\n",
      "Epoch is 49\n",
      "Number of batches 3\n",
      "batch 0 d_loss : 0.391682\n",
      "batch 0 g_loss : 2.078125\n",
      "batch 1 d_loss : 0.420894\n",
      "batch 1 g_loss : 1.582403\n",
      "batch 2 d_loss : 0.272361\n",
      "batch 2 g_loss : 2.146583\n"
     ]
    }
   ],
   "source": [
    "train(50, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=1024)`\n",
      "  app.launch_new_instance()\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), padding=\"same\")`\n",
      "/Users/luli/miniconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (5, 5), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "(1, 1, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "generate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
